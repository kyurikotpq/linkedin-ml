{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./labelled_posts.csv\")\n",
    "\n",
    "# Drop reposts (i.e. engagement & comments = 0)\n",
    "no_reposts_df = df[(df['engagement'] != 0) | (df['comments'] != 0)].copy()\n",
    "\n",
    "# Lowercase all words\n",
    "no_reposts_df.loc[:, \"content\"] = no_reposts_df[\"content\"].apply(lambda x : str.lower(x).replace(\"\\n\\n\", \"\").replace(\"\\n\", \"\"))\n",
    "\n",
    "\n",
    "texts, labels = no_reposts_df[\"content\"], no_reposts_df[\"personal_exp\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, Sequential\n",
    "from tensorflow import expand_dims\n",
    "\n",
    "max_features = no_reposts_df[\"content\"].str.len().max()\n",
    "sequence_length = int(0.01 * max_features)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    texts, labels, test_size=0.2, random_state=21)\n",
    "\n",
    "vectorize_layer = layers.TextVectorization(\n",
    "    max_tokens=max_features,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=sequence_length)\n",
    "\n",
    "vectorize_layer.adapt(X_train)\n",
    "\n",
    "\n",
    "def vectorize_text(text):\n",
    "    text = expand_dims(text, -1)\n",
    "    return vectorize_layer(text)\n",
    "\n",
    "\n",
    "X_train_vectorized = vectorize_layer(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 27, 16)            44608     \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 432)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                27712     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 74433 (290.75 KB)\n",
      "Trainable params: 74433 (290.75 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential([\n",
    "    # layers.Embedding(max_features + 1, 16),\n",
    "    # layers.GlobalAveragePooling1D(),\n",
    "    # layers.Dense(1)\n",
    "    layers.Embedding(max_features + 1, 16, input_length=sequence_length),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "]\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer='adam',\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "6/6 [==============================] - 0s 2ms/step - loss: 0.6843 - accuracy: 0.6587\n",
      "Epoch 2/12\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.6568 - accuracy: 0.6886\n",
      "Epoch 3/12\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.6255 - accuracy: 0.6886\n",
      "Epoch 4/12\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5872 - accuracy: 0.6946\n",
      "Epoch 5/12\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.5358 - accuracy: 0.6946\n",
      "Epoch 6/12\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4734 - accuracy: 0.7186\n",
      "Epoch 7/12\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.4055 - accuracy: 0.7545\n",
      "Epoch 8/12\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.3312 - accuracy: 0.8683\n",
      "Epoch 9/12\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.2563 - accuracy: 0.9641\n",
      "Epoch 10/12\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1877 - accuracy: 1.0000\n",
      "Epoch 11/12\n",
      "6/6 [==============================] - 0s 1ms/step - loss: 0.1292 - accuracy: 1.0000\n",
      "Epoch 12/12\n",
      "6/6 [==============================] - 0s 985us/step - loss: 0.0824 - accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "epochs = 12\n",
    "history = model.fit(\n",
    "    X_train_vectorized, y_train,\n",
    "    epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 2ms/step - loss: 0.5057 - accuracy: 0.7619\n",
      "Test Accuracy: 0.761904776096344\n"
     ]
    }
   ],
   "source": [
    "X_test_vectorized = vectorize_layer(X_test)\n",
    "loss, accuracy = model.evaluate(X_test_vectorized, y_test)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step - loss: 1.0072 - accuracy: 0.2727\n",
      "Test Accuracy: 0.27272728085517883\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./my_posts_labelled.csv\")\n",
    "\n",
    "# Drop reposts (i.e. engagement & comments = 0)\n",
    "no_reposts_df = df[(df['engagement'] != 0) | (df['comments'] != 0)].copy()\n",
    "\n",
    "# Lowercase all words\n",
    "no_reposts_df.loc[:, \"content\"] = no_reposts_df[\"content\"].apply(lambda x : str.lower(x).replace(\"\\n\\n\", \"\").replace(\"\\n\", \"\"))\n",
    "\n",
    "\n",
    "texts, labels = no_reposts_df[\"content\"], no_reposts_df[\"personal_exp\"]\n",
    "X_test_vectorized = vectorize_layer(texts)\n",
    "loss, accuracy = model.evaluate(X_test_vectorized, labels)\n",
    "print(\"Test Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From ChatGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'gensim' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kyurikotpq/Documents/Projects/linkedin-ml/classification/neural-network.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kyurikotpq/Documents/Projects/linkedin-ml/classification/neural-network.ipynb#W0sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m X \u001b[39m=\u001b[39m pad_sequences(sequences, maxlen\u001b[39m=\u001b[39m\u001b[39m20\u001b[39m)  \u001b[39m# Adjust maxlen as needed\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kyurikotpq/Documents/Projects/linkedin-ml/classification/neural-network.ipynb#W0sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Load and train Word2Vec model (replace 'sentences' with your dataset)\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kyurikotpq/Documents/Projects/linkedin-ml/classification/neural-network.ipynb#W0sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m word2vec_model \u001b[39m=\u001b[39m gensim\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mWord2Vec(sentences\u001b[39m=\u001b[39msequences, vector_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m, window\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, min_count\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, sg\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kyurikotpq/Documents/Projects/linkedin-ml/classification/neural-network.ipynb#W0sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# Create an embedding matrix\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kyurikotpq/Documents/Projects/linkedin-ml/classification/neural-network.ipynb#W0sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m word_index \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39mword_index\n",
      "\u001b[0;31mNameError\u001b[0m: name 'gensim' is not defined"
     ]
    }
   ],
   "source": [
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Tokenization and Padding\n",
    "tokenizer = Tokenizer(num_words=1000)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "X = pad_sequences(sequences, maxlen=20)  # Adjust maxlen as needed\n",
    "\n",
    "# Load and train Word2Vec model (replace 'sentences' with your dataset)\n",
    "word2vec_model = gensim.models.Word2Vec(sentences=sequences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Create an embedding matrix\n",
    "word_index = tokenizer.word_index\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 100))  # Embedding size is 100\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model:\n",
    "        embedding_matrix[i] = word2vec_model[word]\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the neural network model\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(word_index) + 1, 100, weights=[embedding_matrix], input_length=20, trainable=False))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=2)\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = [1 if pred > 0.5 else 0 for pred in y_pred]\n",
    "\n",
    "# Convert one-hot encoded labels back to original labels\n",
    "y_test_original = label_encoder.inverse_transform(y_test)\n",
    "y_pred_original = label_encoder.inverse_transform(y_pred_binary)\n",
    "\n",
    "# Evaluate and print model performance\n",
    "accuracy = accuracy_score(y_test_original, y_pred_original)\n",
    "conf_matrix = confusion_matrix(y_test_original, y_pred_original)\n",
    "classification_rep = classification_report(y_test_original, y_pred_original)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Confusion Matrix:\\n\", conf_matrix)\n",
    "print(\"Classification Report:\\n\", classification_rep)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

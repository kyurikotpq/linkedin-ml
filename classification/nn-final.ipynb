{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"./labelled_posts.csv\")\n",
    "\n",
    "# Drop reposts (i.e. engagement == 0 && comments == 0)\n",
    "no_reposts_df = df[(df['engagement'] != 0) | (df['comments'] != 0)].copy()\n",
    "\n",
    "texts, labels = no_reposts_df[\"content\"], no_reposts_df[\"personal_exp\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras_tuner as kt\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras import layers, Sequential, optimizers\n",
    "\n",
    "\n",
    "class LIPostClassifier:\n",
    "    def __init__(self, inputs, targets, kfold_num=5) -> None:\n",
    "        self.MAX_EPOCHS = 50\n",
    "\n",
    "        self.max_features = inputs.str.len().max()\n",
    "        self.sequence_length = int(0.01 * self.max_features)\n",
    "        self.vectorizer = None\n",
    "\n",
    "        self.inputs = inputs.to_numpy()\n",
    "        self.targets = targets.to_numpy()\n",
    "        self.kfolder = KFold(n_splits=kfold_num, shuffle=True, random_state=42)\n",
    "\n",
    "    def create_hypermodel(self, hp):\n",
    "        HP_OUTPUT_DIM = hp.Int('output_dim', min_value=16,\n",
    "                               max_value=80, step=16)\n",
    "\n",
    "        model = Sequential()\n",
    "        model.add(layers.Embedding(self.max_features + 1,\n",
    "                  HP_OUTPUT_DIM, input_length=self.sequence_length))\n",
    "        model.add(layers.Flatten())\n",
    "\n",
    "        # # Embedding Layer for our data\n",
    "        # layers.Embedding(self.max_features + 1, 64,\n",
    "        #                  input_length=self.sequence_length),\n",
    "        #     layers.Flatten(),\n",
    "\n",
    "        #     # Hidden layers\n",
    "        #     layers.Dense(units=HP_INITIAL_DIM, activation='relu'),\n",
    "        #     layers.Dense(units=hp.Int('units', min_value=16,\n",
    "        #                  max_value=128, step=16), activation='relu'),\n",
    "\n",
    "        #     # Output Layer\n",
    "        #     layers.Dense(1, activation='sigmoid')\n",
    "        # ])\n",
    "\n",
    "        for i in range(hp.Int(\"num_layers\", 1, 3)):\n",
    "            model.add(\n",
    "                layers.Dense(\n",
    "                    # Tune number of units separately.\n",
    "                    units=hp.Int(f\"units_{i}\", min_value=16,\n",
    "                                 max_value=128, step=16),\n",
    "                    activation=\"relu\"\n",
    "                )\n",
    "            )\n",
    "\n",
    "        model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.compile(loss=\"binary_crossentropy\",\n",
    "                      optimizer=optimizers.legacy.SGD(\n",
    "                          learning_rate=0.1),\n",
    "                      metrics=[\"accuracy\"])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def model_eval(self):\n",
    "        tuner = kt.BayesianOptimization(self.create_hypermodel,\n",
    "                                        objective='val_accuracy',\n",
    "                                        directory=\"archive\",\n",
    "                                        project_name=\"bayes\",\n",
    "                                        overwrite=True\n",
    "                                        )\n",
    "\n",
    "        # K-fold Cross Validation model evaluation\n",
    "        fold_no = 1\n",
    "\n",
    "        # Define per-fold score containers\n",
    "        acc_per_fold = []\n",
    "        loss_per_fold = []\n",
    "\n",
    "        for train, val in self.kfolder.split(self.inputs, self.targets):\n",
    "            # Learn the vocabulary of the training data\n",
    "            self.vectorizer = layers.TextVectorization(\n",
    "                max_tokens=self.max_features,\n",
    "                output_mode='int',\n",
    "                output_sequence_length=self.sequence_length)\n",
    "            self.vectorizer.adapt(self.inputs[train])\n",
    "\n",
    "            # If not using vectorization layer\n",
    "            X_train = self.vectorizer(self.inputs[train])\n",
    "            y_train = self.targets[train]\n",
    "            tuner.search(X_train, y_train,\n",
    "                         validation_split=0.2, verbose=False)\n",
    "\n",
    "            # Get the optimal hyperparameters\n",
    "            best_hps = tuner.get_best_hyperparameters()[0]\n",
    "\n",
    "            # print(f\"\"\"\n",
    "            # The hyperparameter search is complete. The optimal number of units in the first densely-connected\n",
    "            # layer is {best_hps.get('units')} and the optimal learning rate for the optimizer\n",
    "            # is {best_hps.get('learning_rate')}.\n",
    "            # \"\"\")\n",
    "\n",
    "            # Build the model with the optimal hyperparameters and train it on the data for 50 epochs\n",
    "            model = tuner.hypermodel.build(best_hps)\n",
    "\n",
    "            history = model.fit(\n",
    "                X_train, y_train, epochs=self.MAX_EPOCHS, validation_split=0.2, verbose=False)\n",
    "\n",
    "            val_acc_per_epoch = history.history['val_accuracy']\n",
    "            best_epoch = val_acc_per_epoch.index(max(val_acc_per_epoch)) + 1\n",
    "\n",
    "            # Retrain the model on the best hyperparams and epoch\n",
    "            hypermodel = tuner.hypermodel.build(best_hps)\n",
    "            hypermodel.fit(X_train, y_train, epochs=best_epoch,\n",
    "                           validation_split=0.2, verbose=False)\n",
    "\n",
    "            X_val = self.vectorizer(self.inputs[val])\n",
    "            y_val = self.targets[val]\n",
    "            scores = hypermodel.evaluate(\n",
    "                X_val, y_val, verbose=False)\n",
    "\n",
    "            acc_per_fold.append(scores[1] * 100)\n",
    "            loss_per_fold.append(scores[0])\n",
    "\n",
    "            # Increase fold number\n",
    "            fold_no = fold_no + 1\n",
    "\n",
    "        # Return the best hyperparameters and their\n",
    "        # corr. accuracy and loss metrics\n",
    "        best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        hp_data = {\n",
    "            \"accuracy\": round(np.mean(acc_per_fold), 3),\n",
    "            \"accuracy_std\": round(np.std(acc_per_fold), 3),\n",
    "            \"loss\": round(np.mean(loss_per_fold), 3),\n",
    "            'output_dim': best_hps.values['output_dim'],\n",
    "            'num_layers': best_hps.values['num_layers'],\n",
    "        }\n",
    "\n",
    "        total_params = best_hps.values['output_dim']\n",
    "        last_units = 0\n",
    "        units = []\n",
    "\n",
    "        for i in range(3):\n",
    "            num_units = 0\n",
    "\n",
    "            try:\n",
    "                if best_hps.values[f\"units_{i}\"]:\n",
    "                    num_units = best_hps.values[f\"units_{i}\"]\n",
    "                    units.append(str(num_units))\n",
    "\n",
    "                    total_params += num_units\n",
    "                    last_units = num_units\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "        hp_data[\"units\"] = units\n",
    "\n",
    "        total_params += last_units + 1\n",
    "        hp_data[\"total_params\"] = total_params\n",
    "\n",
    "        return hp_data\n",
    "\n",
    "    def find_best_model(self, tries):\n",
    "        data = []\n",
    "        for _ in range(tries):\n",
    "            data.append(self.model_eval())\n",
    "\n",
    "        print(\"Accuracy (± std)\\t\", \"Loss\\t\", \"Embedding Dim.\\t\",\n",
    "              \"No. of Layers\\t\", \"Total Params\\t\", \"Hidden Layers\\t\")\n",
    "        for hp_data in data:\n",
    "            print(f\"{hp_data['accuracy']} (±{hp_data['accuracy_std']})\\t\\t\",\n",
    "                  f\"{hp_data['loss']}\\t\",\n",
    "                  f\"{hp_data['output_dim']}\\t\\t\",\n",
    "                  f\"{hp_data['num_layers']}\\t\\t\",\n",
    "                  f\"{hp_data['total_params']}\\t\\t\",\n",
    "                  f\"{' → '.join(hp_data['units'])}\\t\",\n",
    "                  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LIPostClassifier(texts, labels)\n",
    "\n",
    "classifier.find_best_model(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{'learning_rate': 0.1, 'output_dim': 32, 'num_layers': 1, 'units_0': 128, 'units_1': 32, 'units_2': 64, 'tuner/epochs': 50, 'tuner/initial_epoch': 0, 'tuner/bracket': 0, 'tuner/round': 0}\n",
    "\n",
    "{'learning_rate': 0.1, 'output_dim': 32, 'num_layers': 1, 'units_0': 128, 'units_1': 32, 'units_2': 64, 'tuner/epochs': 50, 'tuner/initial_epoch': 0, 'tuner/bracket': 0, 'tuner/round': 0}\n",
    "\n",
    "```\n",
    "{'learning_rate': 0.1, 'output_dim': 48, 'num_layers': 2, 'units_0': 80, 'units_1': 112, 'units_2': 96, 'tuner/epochs': 50, 'tuner/initial_epoch': 0, 'tuner/bracket': 0, 'tuner/round': 0}\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " embedding (Embedding)       (None, 29, 48)            139584    \n",
    "                                                                 \n",
    " flatten (Flatten)           (None, 1392)              0         \n",
    "                                                                 \n",
    " dense (Dense)               (None, 80)                111440    \n",
    "                                                                 \n",
    " dense_1 (Dense)             (None, 112)               9072      \n",
    "                                                                 \n",
    " dense_2 (Dense)             (None, 1)                 113       \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 260209 (1016.44 KB)\n",
    "Trainable params: 260209 (1016.44 KB)\n",
    "Non-trainable params: 0 (0.00 Byte)\n",
    "```\n",
    "\n",
    "```\n",
    "{'learning_rate': 0.05, 'output_dim': 80, 'num_layers': 2, 'units_0': 128, 'units_1': 32, 'units_2': 80, 'tuner/epochs': 50, 'tuner/initial_epoch': 0, 'tuner/bracket': 0, 'tuner/round': 0}\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " embedding (Embedding)       (None, 29, 80)            232640    \n",
    "                                                                 \n",
    " flatten (Flatten)           (None, 2320)              0         \n",
    "                                                                 \n",
    " dense (Dense)               (None, 128)               297088    \n",
    "                                                                 \n",
    " dense_1 (Dense)             (None, 32)                4128      \n",
    "                                                                 \n",
    " dense_2 (Dense)             (None, 1)                 33        \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 533889 (2.04 MB)\n",
    "Trainable params: 533889 (2.04 MB)\n",
    "Non-trainable params: 0 (0.00 Byte)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "```\n",
    "Average scores for all folds:\n",
    "> Accuracy: 75.868079662323 (+- 4.182610074413319)\n",
    "> Loss: 0.6991891145706177\n",
    "------------------------------------------------------------------------\n",
    "{'learning_rate': 0.1, 'output_dim': 48, 'num_layers': 3, 'units_0': 112, 'units_1': 32, 'units_2': 128, 'tuner/epochs': 50, 'tuner/initial_epoch': 0, 'tuner/bracket': 0, 'tuner/round': 0}\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " embedding (Embedding)       (None, 29, 48)            139584    \n",
    "                                                                 \n",
    " flatten (Flatten)           (None, 1392)              0         \n",
    "                                                                 \n",
    " dense (Dense)               (None, 112)               156016    \n",
    "                                                                 \n",
    " dense_1 (Dense)             (None, 32)                3616      \n",
    "                                                                 \n",
    " dense_2 (Dense)             (None, 128)               4224      \n",
    "                                                                 \n",
    " dense_3 (Dense)             (None, 1)                 129       \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 303569 (1.16 MB)\n",
    "Trainable params: 303569 (1.16 MB)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```\n",
    "Average scores for all folds:\n",
    "> Accuracy: 74.80519533157349 (+- 1.2396930942487876)\n",
    "> Loss: 0.6298975884914398\n",
    "------------------------------------------------------------------------\n",
    "{'learning_rate': 0.1, 'output_dim': 80, 'num_layers': 3, 'units_0': 80, 'units_1': 64, 'units_2': 64, 'tuner/epochs': 50, 'tuner/initial_epoch': 17, 'tuner/bracket': 2, 'tuner/round': 2, 'tuner/trial_id': '0068'}\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " embedding (Embedding)       (None, 29, 80)            232640    \n",
    "                                                                 \n",
    " flatten (Flatten)           (None, 2320)              0         \n",
    "                                                                 \n",
    " dense (Dense)               (None, 80)                185680    \n",
    "                                                                 \n",
    " dense_1 (Dense)             (None, 64)                5184      \n",
    "                                                                 \n",
    " dense_2 (Dense)             (None, 64)                4160      \n",
    "                                                                 \n",
    " dense_3 (Dense)             (None, 1)                 65        \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 427729 (1.63 MB)\n",
    "Trainable params: 427729 (1.63 MB)\n",
    "Non-trainable params: 0 (0.00 Byte)\n",
    "```\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "```\n",
    "Average scores for all folds:\n",
    "> Accuracy: 77.69993305206299 (+- 4.263463988494678)\n",
    "> Loss: 0.527130925655365\n",
    "{'output_dim': 16, 'num_layers': 1, 'units_0': 64}\n",
    "Model: \"sequential\"\n",
    "_________________________________________________________________\n",
    " Layer (type)                Output Shape              Param #   \n",
    "=================================================================\n",
    " embedding (Embedding)       (None, 29, 16)            46528     \n",
    "                                                                 \n",
    " flatten (Flatten)           (None, 464)               0         \n",
    "                                                                 \n",
    " dense (Dense)               (None, 64)                29760     \n",
    "                                                                 \n",
    " dense_1 (Dense)             (None, 1)                 65        \n",
    "                                                                 \n",
    "=================================================================\n",
    "Total params: 76353 (298.25 KB)\n",
    "Trainable params: 76353 (298.25 KB)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
